# Retrieval-Aware Tool-Using Agent Framework with MCP Integration

A production-ready system combining **Retrieval-Augmented Generation (RAG)**, **Model Context Protocol (MCP)** tools, and **autonomous agent orchestration** to create an intelligent assistant that retrieves context and invokes specialized tools.

## ðŸŽ¯ Features

- **RAG System**: Retrieves relevant documents using HuggingFace embeddings and Chroma vector store
- **MCP Tools** (deployed to Google Cloud Run):
  - ðŸŽ¨ `plot-service`: Mathematical functions (sin, cos, tan, etc.) + categorical visualizations
  - ðŸ”¢ `calculator`: Safe mathematical expression evaluation
  - ðŸ“„ `pdf-parser`: Extract text from PDF documents
- **Dual-Mode Agent Orchestration**:
  - âš¡ **Keyword-based** (517ms avg): Fast, deterministic, zero cost
  - ðŸ§  **Local LLM with TinyLlama** (1.9s avg): Optimized inference, no API needed
  - ðŸ¤– **OpenAI GPT-3.5** (optional): Highest accuracy, requires paid API
- **Interactive UI**: Streamlit frontend for real-time interaction
- **Production-Ready**: Deployed to Cloud Run, 100% free tier compatible
- **Optimized Performance**: 13.7x LLM speedup (26s â†’ 1.9s)
- **Error Handling**: Robust retry logic and graceful error recovery
- **CI/CD**: Automated testing with GitHub Actions

## ðŸ“‹ Architecture

```
[User UI (Streamlit)]
		  â†“
[Agent Orchestrator]
	â†“          â†“           â†“
[RAG]    [Reasoner]  [MCP Tools]
	â†“                      â†‘
[Chroma DB]      [FastAPI Services]
```

## ðŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Git

### Installation

```bash
# Clone repository
git clone https://github.com/Atatsuki2000/Retrieval-Aware-Tool-Using-Agent-Framework-with-MCP-Integration.git
cd Retrieval-Aware-Tool-Using-Agent-Framework-with-MCP-Integration

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# or: source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### Quick Start

**Option 1: Docker Compose (Easiest)**
```bash
# Start all services with Docker
docker-compose up -d --build

# Verify services are running
docker-compose ps

# View logs
docker-compose logs -f
```

Services available at:
- plot-service: http://localhost:8000
- calculator: http://localhost:8001
- pdf-parser: http://localhost:8002

See [DOCKER.md](DOCKER.md) for full Docker guide.

**Option 2: PowerShell/Bash Scripts**
```bash
# Windows
.\start_services.ps1

# Linux/Mac
chmod +x start_services.sh
./start_services.sh
```

This will automatically start all MCP services and the Streamlit UI.

**Option 3: Manual Startup**

1. **Start MCP Tool Services** (in separate terminals):

```bash
# Terminal 1: Plot Service
cd tools/plot-service
uvicorn main:app --host 0.0.0.0 --port 8000

# Terminal 2: Calculator Service
cd tools/calculator
uvicorn main:app --host 0.0.0.0 --port 8001

# Terminal 3: PDF Parser Service
cd tools/pdf-parser
uvicorn main:app --host 0.0.0.0 --port 8002
```

2. **Launch Streamlit UI**:

```bash
cd frontend
streamlit run app.py --server.port 9000
```

3. **Configure Endpoints** in the Streamlit sidebar:
	- plot-service URL: `http://127.0.0.1:8000/mcp/plot`
	- calculator URL: `http://127.0.0.1:8001/mcp/calculate`
	- pdf-parser URL: `http://127.0.0.1:8002/mcp/parse`

4. **Try Example Queries**:
	- "Plot sin(x) from 0 to 10" (mathematical visualization)
	- "Calculate 25 * 17 + 89" (calculator tool)
	- "Show me a bar chart" (categorical visualization)
	- "What is machine learning?" (RAG retrieval only)

## ðŸ§ª Testing

### Unit & Integration Tests

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=agent --cov-report=html
```

### Performance Benchmarks

```bash
# Run all benchmarks (retrieval + agent modes comparison)
python benchmark.py --mode all --save

# Test only retrieval performance
python benchmark.py --mode retrieval

# Compare agent modes (keyword vs local LLM vs OpenAI)
python benchmark.py --mode comparison --save
```

**Benchmark Metrics:**
- ðŸ“Š **Retrieval Precision@k**: Accuracy of document retrieval
- â±ï¸ **Latency Breakdown**: Retrieval, LLM, tool invocation timings
- ðŸŽ¯ **Tool Selection Accuracy**: Correctness of agent's tool choice
- âœ… **Tool Success Rate**: Percentage of successful MCP calls

See [Benchmarking Guide](docs/benchmarking.md) for detailed usage and interpretation.

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ agent/              # Agent orchestration & retriever
â”‚   â”œâ”€â”€ agent.py        # Main agent logic
â”‚   â”œâ”€â”€ retriever.py    # RAG retrieval system
â”‚   â””â”€â”€ test_corpus.txt # Sample corpus
â”œâ”€â”€ tools/              # MCP tool services
â”‚   â”œâ”€â”€ plot-service/   # Visualization tool
â”‚   â”œâ”€â”€ calculator/     # Math evaluation tool
â”‚   â””â”€â”€ pdf-parser/     # PDF text extraction
â”œâ”€â”€ frontend/           # Streamlit UI
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ tests/              # Integration tests
â”œâ”€â”€ .github/workflows/  # CI/CD pipelines
â””â”€â”€ requirements.txt    # Python dependencies
```

## ðŸ“š Documentation

- **[Architecture Guide](docs/architecture.md)**: System design and data flow diagrams
- **[Deployment Guide](docs/deployment.md)**: Local and Cloud Run deployment instructions
- **[Testing Guide](docs/testing.md)**: Test execution and coverage reporting
- **[Benchmarking Guide](docs/benchmarking.md)**: ðŸ“Š Performance evaluation and metrics
- **[Usage Examples](docs/usage.md)**: API examples and common patterns
- **[Local LLM Setup](docs/local-llm-setup.md)**: ðŸ†“ Use free HuggingFace models for tool selection (no API costs!)
- **[LLM Tool Selection](docs/llm-tool-selection.md)**: Guide to using OpenAI GPT-3.5 (paid API)

## ï¿½ðŸ› ï¸ Development

### Environment Variables

Set these to avoid manual configuration:

```bash
export PLOT_SERVICE_URL=http://127.0.0.1:8000/mcp/plot
export CALCULATOR_URL=http://127.0.0.1:8001/mcp/calculate
export PDF_PARSER_URL=http://127.0.0.1:8002/mcp/parse
```

### Adding New MCP Tools

1. Create a new directory under `tools/`
2. Implement FastAPI endpoint with MCP schema
3. Update agent keyword detection in `agent/agent.py`
4. Add endpoint to Streamlit sidebar configuration

## ðŸ“Š Metrics & Performance

### Benchmarked Performance (Latest Results - 2025)

| Metric | Keyword Mode | Local LLM Mode | Target | Status |
|--------|-------------|----------------|--------|--------|
| **Tool Selection Accuracy** | 100% | 100% | >85% | âœ… Excellent |
| **Tool Success Rate** | 100% | 100% | >90% | âœ… Excellent |
| **Avg Retrieval Latency** | 66ms | 70ms | <100ms | âœ… Excellent |
| **Avg End-to-End Latency** | **517ms** | **1.9s** | <2s | âœ… Excellent |
| **Cost per Query** | $0 | $0 | Free | âœ… Zero Cost |

**Performance Optimization:** TinyLlama optimized from 26s â†’ 1.9s (13.7x speedup) through:
- Simplified prompt design (JSON â†’ direct keyword format)
- Reduced token generation (max_new_tokens: 50 â†’ 10)
- Greedy decoding for faster inference
- Efficient keyword extraction from generated text

### Mode Comparison

| Mode | Accuracy | Latency | Cost | Best For |
|------|----------|---------|------|----------|
| **Keyword** â­ | 100% | 517ms | $0 | Production, ultra-low latency |
| **Local LLM (TinyLlama)** ðŸš€ | 100% | 1.9s | $0 | Zero-cost inference, portfolio demos |
| **OpenAI GPT-3.5** | 95-98% | ~800ms | ~$0.0004 | Highest flexibility |

Run `python benchmark.py --mode comparison --save` for detailed analysis.

## â˜ï¸ Cloud Deployment

### Google Cloud Run (Production)

All 3 MCP tools are deployed to Google Cloud Run (us-central1):
- **plot-service**: https://plot-service-347876502362.us-central1.run.app
- **calculator**: https://calculator-h7whjphxza-uc.a.run.app
- **pdf-parser**: https://pdf-parser-h7whjphxza-uc.a.run.app

**Deployment Cost:** $0/month (100% within free tier)
- 2M requests/month free
- 360k GB-seconds compute free
- 0.5GB container storage free

**To use Cloud Run endpoints in Streamlit:**
```bash
# Set environment variables
export PLOT_SERVICE_URL=https://plot-service-347876502362.us-central1.run.app/mcp/plot
export CALCULATOR_URL=https://calculator-h7whjphxza-uc.a.run.app/mcp/calculate
export PDF_PARSER_URL=https://pdf-parser-h7whjphxza-uc.a.run.app/mcp/parse

# Or configure directly in Streamlit sidebar
```

**Deploy your own:**
```bash
cd tools/plot-service
gcloud run deploy plot-service --source . --region us-central1 --allow-unauthenticated
```

See [Deployment Guide](docs/deployment.md) for detailed instructions.

## ðŸ”’ Security

- No hardcoded credentials (environment variables only)
- Safe expression evaluation using `numexpr`
- Input validation on all MCP endpoints
- HTTPS recommended for production deployments

## ðŸ“ License

MIT License

## ðŸ¤ Contributing

Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) for details on:
- Development setup
- Code style guidelines
- Testing requirements
- Pull request process

## ðŸ“œ Changelog

See [CHANGELOG.md](CHANGELOG.md) for version history and release notes.

## ðŸ™ Acknowledgments

Built with:
- [LangChain](https://github.com/langchain-ai/langchain)
- [Chroma](https://github.com/chroma-core/chroma)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Streamlit](https://streamlit.io/)
- [HuggingFace](https://huggingface.co/)

## ðŸ“§ Contact

- **GitHub Issues**: For bug reports and feature requests
- **Repository**: https://github.com/Atatsuki2000/Retrieval-Aware-Tool-Using-Agent-Framework-with-MCP-Integration

## â­ Show Your Support

If you find this project helpful, please consider giving it a star on GitHub!
